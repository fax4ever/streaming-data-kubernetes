#+MACRO: cluster cluster-streaming-v3
#+MACRO: ip 35.205.20.82

* Variables
** OpenShift cluster name
{{{cluster}}}
** OpenShift master IP address
{{{ip}}}
* Home golders
** OpenShifter set up
#+BEGIN_SRC shell
~/0/events/jfokus-18/openshifter
#+END_SRC
** Live coding
#+BEGIN_SRC shell
~/1/streaming-data-kubernetes
#+END_SRC
* First time
** Create project on GCP
#+BEGIN_SRC shell
cd openshifter
google-cloud
./create_project.sh os-v2 galder@zamarreno.com
#+END_SRC
** Create OpenShift cluster on project
#+BEGIN_SRC shell
cd openshifter
docker ps -a | awk '{ print $1,$2 }' | grep openshifter | awk '{print $1 }' | xargs -I {} docker rm {}
docker rmi osevg/openshifter
docker run -e -ti -v `pwd`:/root/data docker.io/osevg/openshifter create {{{cluster}}}
#+END_SRC
** If something fails, destroy the cluster and then recreate it
#+BEGIN_SRC shell
docker run -e -ti -v `pwd`:/root/data docker.io/osevg/openshifter destroy {{{cluster}}}
./delete-resources.sh {{{cluster}}}
docker ps -a | awk '{ print $1,$2 }' | grep openshifter | awk '{print $1 }' | xargs -I {} docker rm {}
docker rmi osevg/openshifter
#+END_SRC
* Pre talk
** Verify if OpenShift master in Google Cloud is responsive
https://console.{{{cluster}}}.{{{ip}}}.nip.io:8443/console/

If not accessible, create a new cluster.
Otherwise cleanup any previous deployments executing:
#+BEGIN_SRC shell
oc loginf -u developer -p developer https://console.{{{cluster}}}.{{{ip}}}.nip.io:8443
oc-cleanup
#+END_SRC
** Login with OpenShift client to Google Cloud
#+BEGIN_SRC shell
oc36
./setup-gcp-openshift.sh {{{ip}}} {{{cluster}}}
#+END_SRC
* Live Coding
** Infinispan data grid
*** Homepage / Select from Project
*** Select ~myproject~
*** Click on ~infinispan-ephemeral~ and click ~Next~
*** Add details
#+BEGIN_SRC shell
APPLICATION_NAME: datagrid
MANAGEMENT_USER: developer
MANAGEMENT_PASSWORD: developer
NUMBER_OF_INSTANCES: 3
#+END_SRC
** Infinispan data grid Visualizer
*** Deploy visualizer
#+BEGIN_SRC shell
cd visual
oc project myproject
oc new-build --binary --name=visual
oc start-build visual --from-dir=. --follow
oc new-app visual
oc expose service visual
#+END_SRC
*** Verify visualizer
http://visual-myproject.apps.{{{cluster}}}.{{{ip}}}.nip.io/infinispan-visualizer/
** Test Infinispan datagrid
*** Create a ~Main~ verticle in app project
**** Implement start() method
#+BEGIN_SRC java
@Override
public void start(io.vertx.core.Future<Void> future) {
  Router router = Router.router(vertx);
  router.get("/test").handler(this::test);

  vertx.createHttpServer()
    .requestHandler(router::accept)
    .rxListen(8080)
    .subscribe(
      server -> {
        log.info("HTTP server started");
        future.complete();
      },
      future::fail
    );
}
#+END_SRC
**** Copy/paste RemoteCacheManager and RemoteCache creation code
Type ~skrcm~ ([[skrcm]]) and press ~TAB~
**** Implement test(RoutingContext) method
#+BEGIN_SRC java
private void test(RoutingContext rc) {
  vertx
    .rxExecuteBlocking(Main::remoteCacheManager)
    .flatMap(remote -> vertx.rxExecuteBlocking(remoteCache(remote)))
    .flatMap(cache -> CompletableInterop.fromFuture(cache.putAsync("hello", "world")).andThen(just(cache)))
    .flatMap(cache -> Single.fromFuture(cache.getAsync("hello")))
    .subscribe(
      value ->
        rc.response().end(value)
      , failure ->
        rc.response().end("Failure: " + failure.toString())
    )
  ;
}
#+END_SRC
*** Build and deploy app project
#+BEGIN_SRC shell
cd app
mvn fabric8:deploy
#+END_SRC
*** Test the application
Switch visualizer to ~repl~ cache.

Switch to terminal and make sure visualizer is in background.

#+BEGIN_SRC shell
curl http://app-myproject.apps.{{{cluster}}}.{{{ip}}}.nip.io/test
#+END_SRC
** Integrate data injector
*** Add a route for /inject and start the Injector verticle
#+BEGIN_SRC java
router.get("/inject").handler(this::inject);
#+END_SRC
#+BEGIN_SRC java
private void inject(RoutingContext rc) {
  vertx
    .rxDeployVerticle(Injector.class.getName(), new DeploymentOptions())
    .subscribe(
      x ->
        rc.response().end("Injector started")
      , failure ->
        rc.response().end("Injector failed to start: " + failure)
    );
}
#+END_SRC
*** Redeploy the app
#+BEGIN_SRC shell
mvn fabric8:deploy
#+END_SRC
*** Start injector
Switch visualizer to default cache.

Switch to terminal and make sure visualizer is in background.

#+BEGIN_SRC shell
curl http://app-myproject.apps.{{{cluster}}}.{{{ip}}}.nip.io/inject
#+END_SRC
** Add Continuous Query Listener
*** Implement continuous query listener
#+BEGIN_SRC java
private void addContinuousQuery(RemoteCache<String, Stop> stopsCache) {
  QueryFactory qf = Search.getQueryFactory(stopsCache);

  Query query = qf.from(Stop.class)
    .having("delayMin").gt(0)
    .build();

  ContinuousQueryListener<String, Stop> listener =
      new ContinuousQueryListener<String, Stop>() {
    @Override
    public void resultJoining(String key, Stop value) {
      vertx.eventBus().publish("delayed-trains", toJson(value));
    }
  };

  continuousQuery = Search.getContinuousQuery(stopsCache);
  continuousQuery.addContinuousQueryListener(query, listener);
}
#+END_SRC
*** Add evenbus route for sending events back to dashboard
#+BEGIN_SRC java
router.get("/eventbus/*").handler(AppUtils.sockJSHandler(vertx));
#+END_SRC
*** Make /inject route deploy the continuous query listener
#+BEGIN_SRC java
.flatMap(x -> vertx.rxDeployVerticle(Listener.class.getName(), new DeploymentOptions()));
#+END_SRC
*** Redeploy the app
#+BEGIN_SRC shell
mvn fabric8:deploy
#+END_SRC
*** Start injector
Switch to terminal and make sure visualizer is in background.

#+BEGIN_SRC shell
curl http://app-myproject.apps.{{{cluster}}}.{{{ip}}}.nip.io/inject
#+END_SRC
*** Run Dashboard from IDE and check that delayed trains are received
Make sure http.host system property correctly points to the address, e.g.
#+BEGIN_SRC shell
-Dhttp.host=app-myproject.apps.{{{cluster}}}.{{{ip}}}.nip.io
#+END_SRC
* Live templates
** skrcm
#+BEGIN_SRC java
private static void remoteCacheManager(Future<RemoteCacheManager> f) {
  f.complete(
    new RemoteCacheManager(
      new ConfigurationBuilder().addServer()
        .host("datagrid-hotrod")
        .port(11222)
        .build()));
}

private static Handler<Future<RemoteCache<String, String>>> remoteCache(RemoteCacheManager remote) {
  return f -> f.complete(remote.getCache("repl"));
}
#+END_SRC
